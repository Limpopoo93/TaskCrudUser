<a href="https://mc.yandex.ru/pixel/8711235002931986822?rnd=%aw_random%">
    <img src="https://mc.yandex.ru/pixel/8711235002931986822?rnd=%aw_random%" />        
  </a>&nbsp;&nbsp;
<a href="https://mc.yandex.ru/watch/92801430">
    <img src="https://mc.yandex.ru/watch/92801430" />        
  </a>&nbsp;&nbsp;

Если вам интересно мое резюме: https://github.com/DEBAGanov

# Kubernetes
- [Kubernetes](#Kubernetes)
  - [Что такое Kubernetes?](#Что_такое_Kubernetes)
  - [Из каких компонентов состоит Kubernetes?](#Из_каких_компонентов_состоит_Kubernetes)
  - [Виды контроллеров?](#Виды_контроллеров)
  - [Что такое Kubernet Cluster?](#Что_такое_Kubernet_Cluster)
  - [Основные Обьекты Kubernetes?](#Основные_Обьекты_Kubernetes)

## Что_такое_Kubernetes

Это платформа с открытым исходным кодом для развертывания, масштабирования, управления и контроля контейнеризованных приложений 
либо сервисов.
Kubernetes делает следующее:
1. Управляет и запускает контейнеры.
2. Балансирует сетевой трафик между узлами кластера Kubernetes и количеством реплик контейнеров.
3. Осуществляет контроль состояния, автоматические развертывания и откаты реплик контейнеров внутри узлов кластера Kubernetes.
4. Осуществляет распределение нагрузки между узлами кластера Kubernetes. Например управление ресурсами при развертывания нового 
   контейнера

## Из_каких_компонентов_состоит_Kubernetes

Основные компоненты Kubernetes
1. Control Plane (Master-нода) – управляет кластером (включает API Server, Scheduler, Controller Manager, etcd).
2. Worker-ноды – рабочие серверы, где запускаются контейнеры (содержат kubelet, kube-proxy, container runtime).
3. Pod – минимальная единица развертывания (может содержать один или несколько контейнеров).
4. Deployment – декларативное описание желаемого состояния приложения.
5. Service – абстракция для доступа к группе подов (например, балансировщик нагрузки).
6. Ingress – управление внешним доступом к сервисам (HTTP/HTTPS-маршрутизация).

## Master_node_и_Worker_node

1. Master-нода (Control Plane)
   Master — это "мозг" кластера Kubernetes. Он управляет всем кластером, принимает решения о развертывании, 
   масштабировании и обеспечивает отказоустойчивость.
   Основные компоненты Master-ноды:
   1. kube-apiserver
      - Что делает? Главный интерфейс для управления кластером.
      - Как работает? Принимает REST-запросы от пользователей (kubectl), валидирует их и обновляет состояние в etcd.
      - Пример: Когда ты запускаешь kubectl create deployment, запрос идёт именно сюда.
   2. etcd
      - Что делает? Распределённое key-value хранилище, где хранится все состояние кластера (конфиги, секреты, метаданные).
      - Как работает? Работает по алгоритму консенсуса Raft (как в Consul). Если etcd падает — кластер становится неуправляемым.
   3. kube-scheduler
      - Что делает? Решает, на какой Worker-ноде запускать под (Pod).
      - Как работает? Анализирует:
        - Достаточно ли на ноде CPU/RAM?
        - Есть ли ограничения (tolerations, node affinity)?
        - Какая нода менее загружена?
   4. kube-controller-manager
      - Что делает? Следит за тем, чтобы текущее состояние кластера соответствовало желаемому (declarative approach).
      - Какие контроллеры внутри?
        - Deployment Controller – следит за репликами подов.
        - Node Controller – реагирует на отвалившиеся ноды.
        - Endpoint Controller – связывает сервисы и поды.

2. Worker-ноды
   Worker — это рабочие серверы, на которых фактически запускаются контейнеры.
   Основные компоненты Worker-ноды
   1. kubelet
      - Что делает? "Агент" на Worker-ноде, который запускает и контролирует поды.
      - Как работает?
        - Получает команды от Master-ноды ("запусти под с таким-то образом").
        - Следит, чтобы контейнеры в подах были живы.
        - Отчитывается Master-ноде о состоянии ноды.
   2. kube-proxy
      - Что делает? Обеспечивает сетевую коммуникацию между сервисами.
      - Как работает?
        - Настраивает правила iptables/IPVS для балансировки нагрузки.
        - Позволяет подам внутри кластера находить друг друга по DNS или ClusterIP.
   3. Container Runtime
      - Что делает? Запускает контейнеры (например, Docker, containerd, CRI-O).
      - Как работает?
        - Получает от kubelet команду: "Запусти контейнер из образа nginx:latest".
        - Скачивает образ и запускает контейнер.

Как Master и Worker взаимодействуют?
- Ты отправляешь команду через kubectl → она попадает в kube-apiserver.
- kube-scheduler решает, на какой Worker-ноде запустить под.
- kubelet на Worker-ноде получает задание и запускает контейнер через Container Runtime.
- kube-proxy настраивает сетевые правила, чтобы под был доступен.
- Controller Manager следит, чтобы всё работало как задумано.

## Pod

Pod — это "обёртка" для одного или нескольких контейнеров, которые:
- Разделяют общие сетевые пространства (один IP-адрес).
- Имеют общие тома хранения (volumes).
- Запускаются и удаляются вместе (как единое целое).

Жизненный цикл Pod'а
- Pending — Kubernetes принимает Pod, но контейнеры ещё не запущены.
- Running — контейнеры работают.
- Succeeded/Failed — контейнеры завершились (успешно или с ошибкой).
- Terminating — Pod удаляется.

У каждого Pod'а есть:
1. Контейнеры (Containers)
   Основная часть Pod'а — это один или несколько контейнеров (например, nginx + sidecar-логгер).
   Контейнеры внутри Pod'а:
     - Общий network namespace → видят друг друга по localhost.
     - Общие volumes → могут обмениваться файлами.
2. Сетевые настройки (Network)
   Каждый Pod получает:
   - Уникальный IP-адрес внутри кластера (из диапазона podCIDR).
   - Общий сетевой namespace для всех контейнеров внутри.
3. Тома (Volumes, опционально)
   Pod может монтировать тома (volumes) для хранения данных:
   - emptyDir (временное хранилище внутри Pod'а).
   - hostPath (доступ к файлам на ноде).
   - PersistentVolume (постоянное хранилище, например, облачный диск).
4. Метаданные (Metadata)
   Включает:
   - Имя Pod'а (metadata.name).
   - Лейблы (metadata.labels), например, app: frontend.
   - Аннотации (metadata.annotations) — дополнительная информация.
5. Spec и Status
   - Spec — желаемое состояние Pod'а (какие контейнеры, volumes, переменные среды и т. д.).
   - Status — текущее состояние (запущен, завершён, ошибка и т. д.).

## Sidecar

Sidecar — это архитектурный паттерн, при котором вспомогательный контейнер (sidecar) работает вместе с основным 
контейнером в одном Pod, расширяя его функциональность без изменения кода основного приложения.

Основная идея:
- Разделение ответственности (основное приложение + вспомогательные задачи).
- Общие ресурсы Pod'а (сеть, volumes, IPC).
- Гибкость — можно добавлять/менять sidecar без переписывания основного кода.

Примеры задач для Sidecar:
- Логирование (отправка логов в ELK, Fluentd).
- Мониторинг (сбор метрик для Prometheus).
- Обновление конфигов (например, Certbot для SSL).
- Сетевые функции (прокси, TLS, балансировка).
- Кеширование (Varnish, Redis).

## loadbalancer

В Kubernetes LoadBalancer — это тип сервиса (Service), который предоставляет внешний доступ к приложению, распределяя трафик 
между подами. Он особенно полезен в облачных средах (AWS, GCP, Azure), где интегрируется с облачными балансировщиками нагрузки.

Как работает LoadBalancer?
- Пользователь отправляет запрос на внешний IP-адрес балансировщика.
- Облачный LB (например, AWS ELB) перенаправляет трафик на ноды Kubernetes.
- Kube-proxy на нодах направляет запросы в поды через iptables/IPVS.
- Поды обрабатывают запрос и возвращают ответ.

## nginx

Nginx — один из самых популярных веб-серверов и обратных прокси, и его интеграция с Kubernetes открывает 
мощные возможности для развертывания, масштабирования и управления веб-приложениями.
Зачем запускать Nginx в Kubernetes?
- Сервинг статики (HTML, CSS, JS, изображения).
- Балансировка нагрузки между микросервисами.
- Терминация TLS (SSL-сертификаты).
- Роутинг трафика (Ingress-контроллер).
- Кеширование и сжатие контента.

## istio (Sidecar контейнер в поде)

Контейнер Istio (istio-proxy, на основе Envoy) внедряется в Pod автоматически при включении Service Mesh в кластере Kubernetes.
Он выполняет критически важные функции по управлению сетевым трафиком, безопасности и observability без изменения кода основного 
приложения.

Основные функции Istio Sidecar
1. Управление трафиком (Traffic Control)
   - Локальная балансировка нагрузки между экземплярами сервиса.
   - A/B-тестирование и канареечные развертывания — маршрутизация части трафика на новую версию приложения.
   - Трафик-политики: ограничение скорости (rate limiting), retry-логика, circuit breaking.
2. Безопасность (Security)
   - Автоматическое mTLS — шифрование трафика между сервисами.
   - Аутентификация и авторизация на уровне сервисов (JWT, OAuth2).
   - Политики доступа (например, запрет доступа из определенных namespace).
3. Наблюдаемость (Observability)
   - Метрики (Prometheus) — latency, ошибки, объем трафика.
   - Трассировка (Jaeger/Zipkin) — распределенные трейсы запросов.
   - Логи (Fluentd) — детализация сетевых событий.
   - Дашборды (Grafana/Kiali) — визуализация сервисных зависимостей.

Как трафик проходит через Sidecar?
- Входящий запрос → Попадает в istio-proxy → Перенаправляется в основной контейнер.
- Исходящий запрос из основного контейнера → Перехватывается istio-proxy → Отправляется наружу.

## Виды_контроллеров

Виды контроллеров:
1. Deployments - контроллер, который управляет состоянием развертывания подов, которое описывается в манифесте, следит за 
   удалением и созданием экземпляров подов. Управляет контроллерами ReplicaSet.
2. ReplicaSet - гарантирует, что определенное количество экземпляров подов всегда будет запущено в кластере.
3. StatefulSets - так же как и Deployments, управляет развертыванием и масштабированием набора подов, но сохраняет набор 
   идентификаторов и состояние для каждого пода.
4. DaemonSet - гарантирует, что на каждом узле кластера будет присутствовать экземпляр пода.
5. Jobs - создает определенное количество подов и смотрит, пока они успешно не завершат работу. Если под завершился с ошибкой, 
   повторяет создание, которое мы описали определенное количество раз. Если под успешно отработал, записывает это в свой журнал.
6. CronJob - запускает контроллеры Jobs по определенному расписанию.

## Что_такое_Kubernet_Cluster

Состоит минимум из одного (для непрерывной и безопасной работы может быть больше), Мастер нода. И минимум один Воркер нод 
(на практике их обычно больше)
В мастер ноде указываем откуда брать Docker Images (из Dockerhub, AWS Container Registry ECR, Google Container Registry, 
Azure Container Registry и тд). Контейнеры разворачиваются на воркер нодах, заполняя память и процессоры. Если ресурсов будет 
не хватать, кубер может предложить масштабироваться, добавив еще воркер нодов.

## Основные_Обьекты_Kubernetes

Контейнер не является обьектом K8S - это обьект Докера):
1. Pod - самый маленький обьект Кубера, который мы можем создать. Состоит из контейнера (обычно) или нескольких контейнеров 
   (если надо). Чаще один, чтобы при масштабировании не плодить лишние копии
2. Deployment - состоит из одного или нескольких одинаковых Подов, даже если в разных Нодах
3. Service - дает доступ к Подам, которые в Деплое, через ClusterIP, NodePort, LoadBalancer, ExternalName
4. Nodes - сервера, где все это работает
5. Cluster - обьединение нодов
6. StatefulSets - Аналог Deployment для хранения состояния. Например: монтирование постоянного хранилища. Сохранять IP 
7. ConfigMaps - Открытые настройки приложения
8. Jobs - единоразовый запуск задачи. Например выполнить скрипты liquibase
9. ReplicaSets - абстракция Pod.
10. Ingress
11. Routes
12. Manifest - декларативный способ описания что Kubernetes должен сделать.

## etcd

etcd — это распределённое key-value хранилище, используемое Kubernetes как основная база данных для хранения всего состояния кластера.
Это критически важный компонент Control Plane, без которого кластер не сможет функционировать.

Основные функции etcd в Kubernetes:
1. Хранение конфигурации кластера
   - Состав нод (Nodes), их статусы.
   - Информация о Pod’ах, Deployments, Services, Secrets, ConfigMaps и других ресурсах.
3. Координация и распределённый консенсус
   - Использует алгоритм Raft для обеспечения согласованности данных между несколькими etcd-нодами.
   - Гарантирует, что все компоненты Kubernetes видят одинаковое состояние кластера.
4. Хранение секретов и настроек
   - Данные из Secrets и ConfigMaps также сохраняются в etcd (в зашифрованном виде, если включено шифрование).
5. Поддержка watch-механизма
   - Компоненты Kubernetes (например, kube-apiserver) подписываются на изменения в etcd и реагируют на них (например, пересоздают 
     Pod при удалении).

Как etcd интегрирован в Kubernetes?
1. kube-apiserver — единственный компонент, который напрямую взаимодействует с etcd.
  - Все запросы (kubectl apply, kubectl delete) проходят через API-сервер и сохраняются в etcd.
  - Контроллеры (например, kube-controller-manager) читают данные из etcd через API-сервер и принимают решения.
2. Key-Value структура
   - Данные хранятся в виде пар ключ-значение.

Как Kubernetes использует etcd?
1. При создании Pod’а:
   - kubectl отправляет запрос в kube-apiserver.
   - API-сервер сохраняет данные Pod’а в etcd.
   - kube-scheduler видит новый Pod в etcd и назначает ему ноду.
2. При обновлении Deployment:
   - Изменение реплик записывается в etcd.
   - kube-controller-manager замечает изменение и создаёт новые Pod’ы.
3. При удалении ресурса:
   - Запись удаляется из etcd.
   - Контроллеры останавливают связанные процессы.

Безопасность etcd
1. Шифрование данных:
   - Можно включить шифрование для Secrets (например, с помощью kms-provider).
2. Аутентификация:
   - etcd поддерживает TLS-сертификаты для клиентов (например, kube-apiserver).
3. Изоляция:
   - Рекомендуется разворачивать etcd на отдельных нодах (не на тех же, где Control Plane).

## Что такое initContainers и зачем они нужны?

InitContainers (инициализационные контейнеры) — это специальные контейнеры, которые запускаются до основных контейнеров в Pod 
и выполняют подготовительные задачи. Они гарантируют, что Pod начнёт работу только после успешного завершения всех init-контейнеров.

Зачем нужны InitContainers?
1. Подготовка среды
   - Скачивание конфигов или секретов (например, из Vault или S3).
   - Настройка прав на файлы (chmod, chown).
   - Ожидание готовности зависимостей (БД, API, очередей).
2. Предварительные проверки
   - Проверка подключения к внешним сервисам.
   - Валидация конфигурации перед запуском основного приложения.
3. Безопасность
   - Загрузка TLS-сертификатов.
   - Инициализация данных (например, расшифровка Secrets).
   - Управление зависимостями
   - Ожидание готовности другой части приложения (например, sidecar-контейнера).

Как работают InitContainers?
- Запускаются строго по порядку (один за другим).
- Если init-контейнер завершается с ошибкой (exit code != 0), Pod переходит в статус Init:Error и перезапускает контейнер 
  (по умолчанию — бесконечно).
- Основные контейнеры запускаются только после успешного завершения всех init-контейнеров.

## Какие типы Service есть в Kubernetes? Чем отличается ClusterIP от NodePort и LoadBalancer?

ClusterIP (по умолчанию)
- Назначение: Внутренний виртуальный IP-адрес, доступный только внутри кластера.
- Доступ: Только из других Pod'ов или сервисов внутри Kubernetes.
- Использование: Для внутренней коммуникации между компонентами (например, между Backend и БД).

NodePort
- Назначение: Открывает статический порт на каждой Node кластера, делая сервис доступным извне.
- Доступ:
  - Внутри кластера (как ClusterIP).
  - Снаружи через <NodeIP>:<NodePort> (порт в диапазоне 30000-32767 по умолчанию).
- Использование: Для тестирования или когда LoadBalancer недоступен.

LoadBalancer
- Назначение: Интегрируется с облачным провайдером (AWS, GCP, Azure) для создания внешнего балансировщика нагрузки.
- Доступ:
  - Внутри кластера (как ClusterIP).
  - Через NodePort (если поддерживается).
  - Через внешний IP балансировщика нагрузки.
- Использование: Для публичных сервисов (веб-приложений, API).


## Как работает kube-proxy?

kube-proxy — это компонент Kubernetes, который работает на каждой ноде кластера и отвечает за маршрутизацию трафика к 
сервисам (Services). Он обеспечивает, чтобы запросы к сервису (например, по ClusterIP, NodePort или LoadBalancer) корректно 
перенаправлялись на подходящие Pod'ы.

Как работает kube-proxy?
kube-proxy поддерживает несколько режимов работы, которые определяют, как трафик попадает к Pod'ам сервиса:

Режимы работы kube-proxy:
1. Userspace (устаревший, почти не используется)
   Как работает:
   - kube-proxy создает прокси-сервер в пользовательском пространстве.
   - Входящие запросы перенаправляются через этот прокси.
   - Медленный, так как требует переключения между ядром и пользовательским пространством.
2. iptables (режим по умолчанию в большинстве кластеров)
   Как работает:
   - kube-proxy настраивает правила iptables в ядре Linux.
   - Когда запрос приходит на ClusterIP или NodePort, iptables автоматически перенаправляет его на один из Pod'ов сервиса 
     (с балансировкой).
   Плюсы:
   - Быстрый (работает на уровне ядра).
   - Нет накладных расходов на пользовательский прокси.
   Минусы:
   - Если первый Pod не отвечает, iptables не пытается перенаправить запрос на другой (нет retry-логики).
3. IPVS (рекомендуется для больших кластеров)
   Как работает:
   - Использует IP Virtual Server (IPVS) — механизм балансировки нагрузки в ядре Linux.
   - Поддерживает различные алгоритмы балансировки (rr — round-robin, lc — least connections, sh — source hashing и др.).
   Плюсы:
   - Более эффективен, чем iptables, при большом количестве сервисов.
   - Поддерживает сложные сценарии балансировки.
   Минусы:
   - Требует загруженного модуля ядра ip_vs.

## Что такое ServiceAccount в Kubernetes?

ServiceAccount — это объект Kubernetes, который предоставляет идентификацию (identity) для Pod'ов и внутрикластерных процессов, 
позволяя им взаимодействовать с API Kubernetes и другими ресурсами.

Зачем нужен ServiceAccount?
- Авторизация Pod'ов: Определяет, какие действия может выполнять Pod внутри кластера (например, читать Secrets, создавать Pod'ы).
- Безопасность: Ограничивает доступ Pod'ов к API Kubernetes на основе ролей (RBAC).
- Интеграция с внешними сервисами: Например, доступ к облачным API (AWS IAM, GCP Service Accounts).

Как работает ServiceAccount?
Каждому Pod'у при создании назначается ServiceAccount (по умолчанию — default).
Учетные данные (token, ca.crt) монтируются в Pod в секрете:
Эти данные используются для аутентификации при обращении к API Kubernetes.

## Как ограничить доступ к Pod'ам с помощью NetworkPolicies?

NetworkPolicies в Kubernetes позволяют контролировать сетевой трафик между Pod'ами, Namespaces и внешними IP-адресами.
Основные концепции NetworkPolicies
PolicyTypes:
Ingress - входящий трафик
Egress - исходящий трафик

Селекторы:
Выбирают Pod'ы, к которым применяется политика
Могут использовать метки (labels) Pod'ов и Namespace'ов

## Как ограничить права контейнера в OpenShift?

1. Security Context Constraints (SCC)
SCC - основной механизм контроля прав в OpenShift, который определяет:
- Какие пользователи/сервисы могут запускать поды
- Какие привилегии доступны контейнерам
2. Security Context в Pod/Deployment
   Вы можете определить права непосредственно в манифесте

## Как собирать логи из Pod'ов?

1. Использование встроенных команд kubectl/oc
2. Сбор логов всех подов в неймспейсе
3. Fluentd/Fluent Bit собирает логи из подов
Elasticsearch хранит логи
Kibana предоставляет интерфейс для анализа
Loki Stack (Grafana Loki)

## Что такое аннотации в Openshift

Аннотации (annotations) в OpenShift — это механизм добавления произвольных метаданных к объектам Kubernetes 
(подам, сервисам, деплойментам и т.д.), которые используются для хранения дополнительной информации, не предназначенной 
для селекции или фильтрации (в отличие от labels).

Основные особенности аннотаций
- Неиндексируемые метаданные: В отличие от labels, аннотации не используются для фильтрации или группировки объектов
- Произвольные данные: Могут содержать любую информацию в формате ключ-значение
- Больший объем: Поддерживают более крупные значения (до 256KB на ключ)
- Широкий спектр использования: Применяются для различных целей — от документации до управления поведением системы

Типичные варианты использования аннотаций
1. Документирование объектов
2. Управление поведением OpenShift
3. Интеграция с инструментами мониторинга
4. Контроль развертывания
5. Работа с сетью

## Что такое labels в Openshift

Labels (метки) в OpenShift (и Kubernetes) — это пары ключ-значение, которые присваиваются объектам (Pods, Services, 
Deployments, Nodes и др.) для их организации, фильтрации и группировки. В отличие от аннотаций, метки предназначены именно
для селекции (выбора) объектов, а не для хранения произвольных метаданных.

Основные особенности Labels:
1. Используются для идентификации и группировки
   - Например, можно пометить все Pod'ы, относящиеся к frontend, backend или prod/stage.
2. Поддерживают селекторы (selectors)
   - Позволяют выбирать объекты по их меткам (kubectl get pods -l app=frontend).
3. Ограничения по формату
   - Ключи и значения должны соответствовать DNS-стандарту.
   - Пример допустимой метки: app.kubernetes.io/name=frontend.
4. Не предназначены для хранения больших данных
   - В отличие от аннотаций, метки должны быть короткими и информативными.

Labels используются для логической группировки объектов (Pods, Services, Nodes).
Селекторы (matchLabels, -l) позволяют фильтровать объекты по меткам.
Рекомендуется использовать стандартные метки (app.kubernetes.io/*).
Аннотации (annotations) — для хранения дополнительной информации, не связанной с выборкой.
